{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs \u00b6 For full documentation visit mkdocs.org . Commands \u00b6 mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout \u00b6 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"0_setup_download/","text":"Setup \u00b6 \u00ab Home 1 - Unix Shell and Bash \u00bb Do not remove this line (it will not be displayed) Windows hosts \u00b6 Install either Git for Windows from https://git-scm.com/download/win OR MobaXterm Home ( Portable or Installer edition) from https://mobaxterm.mobatek.net/download-home-edition.html Portable edition does not require administrative privileges MacOS and Linux hosts \u00b6 No additional installs required. Download data \u00b6 Download via Web Browser \u00b6 Data can be downloaded directly from this link which will download bashfbio_v1.0_data.tar.gz to Downloads directory If the above link fails, try this alternative .zip Download with wget from a terminal client \u00b6 $ wget -c https://github.com/GenomicsAotearoa/bash-for-bioinformatics/releases/download/v1.0rc1/data_shell4b.tar.gz OR $ wget -c https://github.com/GenomicsAotearoa/bash-for-bioinformatics/releases/download/v1.0rc1/data_shell4b.zip Testing a content tab \u00b6 C C++ R #include <stdio.h> int main ( void ) #include <iostream> int main ) void ) png ( filename = \"plot.png\" ) # This line redirects plots from screen to plot.png file. # Define the cars vector with 5 values cars <- c ( 1 , 3 , 6 , 4 , 9 ) # Graph the cars vector with all defaults plot ( cars ) \u00ab Home 1 - Unix Shell and Bash \u00bb","title":"Setup"},{"location":"0_setup_download/#setup","text":"\u00ab Home 1 - Unix Shell and Bash \u00bb Do not remove this line (it will not be displayed)","title":"Setup"},{"location":"0_setup_download/#windows-hosts","text":"Install either Git for Windows from https://git-scm.com/download/win OR MobaXterm Home ( Portable or Installer edition) from https://mobaxterm.mobatek.net/download-home-edition.html Portable edition does not require administrative privileges","title":"Windows hosts"},{"location":"0_setup_download/#macos-and-linux-hosts","text":"No additional installs required.","title":"MacOS and Linux hosts"},{"location":"0_setup_download/#download-data","text":"","title":"Download data"},{"location":"0_setup_download/#download-via-web-browser","text":"Data can be downloaded directly from this link which will download bashfbio_v1.0_data.tar.gz to Downloads directory If the above link fails, try this alternative .zip","title":"Download via Web Browser"},{"location":"0_setup_download/#download-with-wget-from-a-terminal-client","text":"$ wget -c https://github.com/GenomicsAotearoa/bash-for-bioinformatics/releases/download/v1.0rc1/data_shell4b.tar.gz OR $ wget -c https://github.com/GenomicsAotearoa/bash-for-bioinformatics/releases/download/v1.0rc1/data_shell4b.zip","title":"Download with wget from a terminal client"},{"location":"0_setup_download/#testing-a-content-tab","text":"C C++ R #include <stdio.h> int main ( void ) #include <iostream> int main ) void ) png ( filename = \"plot.png\" ) # This line redirects plots from screen to plot.png file. # Define the cars vector with 5 values cars <- c ( 1 , 3 , 6 , 4 , 9 ) # Graph the cars vector with all defaults plot ( cars ) \u00ab Home 1 - Unix Shell and Bash \u00bb","title":"Testing a content tab"},{"location":"1_introduction/","text":"What is a Unix Shell and Bash \u00b6 \u00ab Setup 3 - Shell Basics and recap \u00bb Do not remove this line (it will not be displayed) Unix Shell \u00b6 A shell is a computer program that presents a command line interface which allows you to control your computer using commands entered with a keyboard instead of controlling graphical user interfaces (GUIs) with a mouse/keyboard/touchscreen combination. There are many reasons to learn about the shell: Many bioinformatics tools can only be used through a command line interface. Many more have features and parameter options which are not available in the GUI. BLAST is an example. Many of the advanced functions are only accessible to users who know how to use a shell. The shell makes your work less boring. In bioinformatics you often need to repeat tasks with a large number of files. With the shell, you can automate those repetitive tasks and leave you free to do more exciting things. The shell makes your work less error-prone. When humans do the same thing a hundred different times (or even ten times), they\u2019re likely to make a mistake. Your computer can do the same thing a thousand times with no mistakes. The shell makes your work more reproducible. When you carry out your work in the command-line (rather than a GUI), your computer keeps a record of every step that you\u2019ve carried out, which you can use to re-do your work when you need to. It also gives you a way to communicate unambiguously what you\u2019ve done, so that others can inspect or apply your process to new data. Many bioinformatic tasks require large amounts of computing power and can\u2019t realistically be run on your own machine. These tasks are best performed using remote computers or cloud computing, which can only be accessed through a shell. Bash \u00b6 Bash (Bourne again shell) is a type of Unix Shell and it is the default shell on most modern implementations of Unix and in most packages that provide Unix-like tools for Windows. \u00ab Setup 3 - Shell Basics and recap \u00bb Back to homepage","title":"What is a Unix Shell and Bash"},{"location":"1_introduction/#what-is-a-unix-shell-and-bash","text":"\u00ab Setup 3 - Shell Basics and recap \u00bb Do not remove this line (it will not be displayed)","title":"What is a Unix Shell and Bash"},{"location":"1_introduction/#unix-shell","text":"A shell is a computer program that presents a command line interface which allows you to control your computer using commands entered with a keyboard instead of controlling graphical user interfaces (GUIs) with a mouse/keyboard/touchscreen combination. There are many reasons to learn about the shell: Many bioinformatics tools can only be used through a command line interface. Many more have features and parameter options which are not available in the GUI. BLAST is an example. Many of the advanced functions are only accessible to users who know how to use a shell. The shell makes your work less boring. In bioinformatics you often need to repeat tasks with a large number of files. With the shell, you can automate those repetitive tasks and leave you free to do more exciting things. The shell makes your work less error-prone. When humans do the same thing a hundred different times (or even ten times), they\u2019re likely to make a mistake. Your computer can do the same thing a thousand times with no mistakes. The shell makes your work more reproducible. When you carry out your work in the command-line (rather than a GUI), your computer keeps a record of every step that you\u2019ve carried out, which you can use to re-do your work when you need to. It also gives you a way to communicate unambiguously what you\u2019ve done, so that others can inspect or apply your process to new data. Many bioinformatic tasks require large amounts of computing power and can\u2019t realistically be run on your own machine. These tasks are best performed using remote computers or cloud computing, which can only be accessed through a shell.","title":"Unix Shell"},{"location":"1_introduction/#bash","text":"Bash (Bourne again shell) is a type of Unix Shell and it is the default shell on most modern implementations of Unix and in most packages that provide Unix-like tools for Windows. \u00ab Setup 3 - Shell Basics and recap \u00bb Back to homepage","title":"Bash"},{"location":"4_inspectmanipluate/","text":"Inspecting and Manipulating Text Data with Unix Tools - Part 1 \u00b6 \u00ab 3 - Shell Basics and recap 5 - Text Manipu. Pt2 \u00bb Do not remove this line (it will not be displayed) Many formats in bioinformatics are simple tabular plain-text files delimited by a character. The most common tabular plain-text file format used in bioinformatics is tab-delimited. Bioinformatics evolved to favor tab-delimited formats because of the convenience of working with these files using Unix tools. Tabular Plain-Text Data Formats : Tabular plain-text data formats are used extensively in computing. The basic format is incredibly simple: each row (also known as a record) is kept on its own line, and each column (also known as a field) is separated by some delimiter. There are three flavors you will encounter: tab-delimited, comma-separated, and variable space-delimited. Inspecting data with head and tail \u00b6 Although cat command is an easy way for us to open and view the content of a file, it is not very practical to do so for a file with thousands of lines as it will exhaust the shell \"space\". Instead, large files should be inspected first and then manipulated accordingly. First round of inspection can be done with head and tail command which prints the first 10 lines and the last 10 lines ( -n 10 ) of a a file, respectively. .i.e. Let's use head and tail to inspect Mus_musculus.GRCm38.75_chr1.bed $ head Mus_musculus.GRCm38.75_chr1.bed 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733 1 3102016 3102125 1 3102016 3102125 1 3102016 3102125 1 3205901 3671498 1 3205901 3216344 1 3213609 3216344 1 3205901 3207317 $ tail Mus_musculus.GRCm38.75_chr1.bed 1 195166217 195166390 1 195165745 195165851 1 195165748 195165851 1 195165745 195165747 1 195228278 195228398 1 195228278 195228398 1 195228278 195228398 1 195240910 195241007 1 195240910 195241007 1 195240910 195241007 Changing the number of lines printed for either of those commands can be done by passing -n <number_of_lines> flag .i.e. Over-ride the -n 10 default Try those commands with 0n 4 to print top 4 lines and bottom 4 lines $ head -n 4 Mus_musculus.GRCm38.75_chr1.bed $ tail -n 4 Mus_musculus.GRCm38.75_chr1.bed Solution Check..Check check the status $ grep awk $ tail -n 4 Mus_musculus.GRCm38.75_chr1.bed $ head -n 4 Mus_musculus.GRCm38.75_chr1.bed Solution_2 Check..Check check the status $ grep awk $ tail -n 4 Mus_musculus.GRCm38.75_chr1.bed $ head -n 4 Mus_musculus.GRCm38.75_chr1.bed Extract summary information with wc \u00b6 The \"wc\" in the wc command which stands for \"word count\" - this command can count the numbers of words, lines , and characters in a file (take a note on the order). $ wc Mus_musculus.GRCm38.75_chr1.bed 81226 243678 1698545 Mus_musculus.GRCm38.75_chr1.bed Often, we only need to list the number of lines, which can be done by using the -l flag. It can be used as a sanity check - for example, to make sure an output has the same number of lines as the input, OR to check that a certain file format which depends on another format without losing overall data structure wasn't corrupted or over/under manipulated. Question - count the number of lines in Mus_musculus.GRCm38.75_chr1.bed and Mus_musculus.GRCm38.75_chr1.gtf . Anything out of the ordinary ? Although wc -l is the quickest way to count the number of lines in a file, it is not the most robust as it relies on the very bad assumption that \"data is well formatted\" For an example, If we are to create a file with 3 rows of data and then two empty lines, $ cat > fool_wc.bed 1 100 2 200 3 300 $ $ wc -l fool_wc.bed 5 fool_wc.bed This is a good place bring in grep again which can be used to count the number of lines while excluding white-spaces (spaces, tabs or newlines) $ grep -c \"[^ \\\\n\\\\t]\" fool_wc.bed 3 Using cut with column data and formatting tabular data with column \u00b6 When working with plain-text tabular data formats like tab-delimited and CSV files, we often need to extract specific columns from the original file or stream. For example, suppose we wanted to extract only the start positions (the second column) of the Mus_musculus.GRCm38.75_chr1.bed file. The simplest way to do this is with cut . $ cut -f 2 Mus_musculus.GRCm38.75_chr1.bed | head -n 3 3054233 3054233 3054233 -f argument is how we specify which columns to keep. It can be used to specify a range as well $ cut -f 2 -3 Mus_musculus.GRCm38.75_chr1.bed | head -n 3 3054233 3054733 3054233 3054733 3054233 3054733 Using cut , we can convert our GTF for Mus_musculus.GRCm38.75_chr1.gtf to a three-column tab-delimited file of genomic ranges (e.g., chromosome, start, and end position). We\u2019ll chop off the metadata rows using the grep command covered earlier, and then use cut to extract the first, fourth, and fifth columns (chromosome, start, end): $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f1,4,5 | head -n 3 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733 Note that although our three-column file of genomic positions looks like a BED- formatted file, it\u2019s not due to subtle differences in genomic range formats cut also allows us to specify the column delimiter character. So, if we were to come across a CSV file containing chromosome names, start positions, and end positions, we could select columns from it, too: As you may have noticed when working with tab-delimited files, it\u2019s not always easy to see which elements belong to a particular column. For example: $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f1-8 | head -n3 1 pseudogene gene 3054233 3054733 . + . 1 unprocessed_pseudogene transcript 3054233 3054733 . + . 1 unprocessed_pseudogene exon 3054233 3054733 . + . While tabs are a great delimiter in plain-text data files, our variable width data leads our columns to not stack up well. There\u2019s a fix for this in Unix: program column -t (the -t option tells column to treat data as a table). column -t produces neat columns that are much easier to read: $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f 1 -8 | column -t | head -n 3 1 pseudogene gene 3054233 3054733 . + . 1 unprocessed_pseudogene transcript 3054233 3054733 . + . 1 unprocessed_pseudogene exon 3054233 3054733 . + . Note that you should only use column -t to visualize data in the terminal, not to reformat data to write to a file. Tab-delimited data is preferable to data delimited by a variable number of spaces, since it\u2019s easier for programs to parse. Like cut , column \u2019s default delimiter is the tab character ( \\t ). We can specify a different delimiter with the -s option. So, if we wanted to visualize the columns of the Mus_musculus.GRCm38.75_chr1_bed.csv file more easily, we could use: $ column -s \",\" -t Mus_musculus.GRCm38.75_chr1.bed | head -n 3 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733 Sorting Plain-Text Data with sort \u00b6 Very often we need to work with sorted plain-text data in bioinformatics. The two most common reasons to sort data are as follows: - Certain operations are much more efficient when performed on sorted data. - Sorting data is a prerequisite to finding all unique lines sort is designed to work with plain-text data with columns. Create a test .bed file with few rows and use sort command without any arguments $ cat > test_sort.bed chr1 26 39 chr1 32 47 chr3 11 28 chr1 40 49 chr3 16 27 chr1 9 28 chr2 35 54 chr1 10 19 $ sort test_sort.bed chr1 10 19 chr1 26 39 chr1 32 47 chr1 40 49 chr1 9 28 chr2 35 54 chr3 11 28 chr3 16 27 sort without any arguments simply sorts a file alphanumerically by line. Because chromosome is the first column, sorting by line effectively groups chromosomes together, as these are \"ties\" in the sorted order. However, using sort \u2019s defaults of sorting alphanumerically by line doesn\u2019t handle tabular data properly. There are two new features we need: The ability to sort by particular columns The ability to tell sort that certain columns are numeric values (and not alpha\u2010numeric text) sort has a simple syntax to do this. Let\u2019s look at how we\u2019d sort example.bed by chromosome (first column), and start position (second column): $ sort -k1,1 -k2n test_sort.bed chr1 9 28 chr1 10 19 chr1 26 39 chr1 32 47 chr1 40 49 chr2 35 54 chr3 11 28 chr3 16 27 Here, we specify the columns (and their order) we want to sort by as -k arguments. In technical terms, -k specifies the sorting keys and their order. Each -k argument takes a range of columns as start,end , so to sort by a single column we use start,start . In the preceding example, we first sorted by the first column (chromosome), as the first -k argument was -k1,1 . Sorting by the first column alone leads to many ties in rows with the same chromosomes (e.g., \u201cchr1\u201d and \u201cchr3\u201d). Adding a second -k argument with a different column tells sort how to break these ties. In our example, -k2,2n tells sort to sort by the second column (start position), treating this column as numerical data (because there\u2019s an n in -k2,2n ). Exercise 4.3 \u00b6 {% capture e4dot3 %} Mus_musculus.GRCm38.75_chr1_random.gtf file is Mus_musculus.GRCm38.75_chr1.gtf with permuted rows (and without a metadata header). Can you group rows by chromosome, and sort by position? If yes, append the output to a separate file. {% endcapture %} {% include exercise.html title=\"e4dot3\" content=e4dot3%} Finding Unique Values with uniq \u00b6 uniq takes lines from a file or standard input stream, and outputs all lines with consecutive duplicates removed. While this is a relatively simple functionality, you will use uniq very frequently in command-line data processing. $ cat letters.txt A A B C B C C C $ uniq letters.txt A B C B C As you can see, uniq does not return the unique values in letters.txt \u2014 it only removes consecutive duplicate lines (keeping one). If instead we did want to find all unique lines in a file, we would first sort all lines using sort so that all identical lines are grouped next to each other, and then run uniq . $ sort letters.txt | uniq A B C uniq with -c shows the counts of occurrences next to the unique lines. $ uniq -c letters.txt 2 A 1 B 1 C 1 B 3 C $ sort letters.txt | uniq -c 2 A 2 B 4 C Combined with other Unix tools like cut , grep and sort , uniq can be used to summarize columns of tabular data: $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f3 | sort | uniq -c 25901 CDS 36128 exon 2027 gene 2290 start_codon 2299 stop_codon 4993 transcript 7588 UTR Count in order from most frequent to last $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f3 | sort | uniq -c | sort -rn 36128 exon 25901 CDS 7588 UTR 4993 transcript 2299 stop_codon 2290 start_codon 2027 gene \u00ab 3 - Shell Basics and recap 5 - Text Manipu. Pt2 \u00bb Back to homepage","title":"Inspecting and Manipulating Text Data with Unix Tools - Part 1"},{"location":"4_inspectmanipluate/#inspecting-and-manipulating-text-data-with-unix-tools-part-1","text":"\u00ab 3 - Shell Basics and recap 5 - Text Manipu. Pt2 \u00bb Do not remove this line (it will not be displayed) Many formats in bioinformatics are simple tabular plain-text files delimited by a character. The most common tabular plain-text file format used in bioinformatics is tab-delimited. Bioinformatics evolved to favor tab-delimited formats because of the convenience of working with these files using Unix tools. Tabular Plain-Text Data Formats : Tabular plain-text data formats are used extensively in computing. The basic format is incredibly simple: each row (also known as a record) is kept on its own line, and each column (also known as a field) is separated by some delimiter. There are three flavors you will encounter: tab-delimited, comma-separated, and variable space-delimited.","title":"Inspecting and Manipulating Text Data with Unix Tools - Part 1"},{"location":"4_inspectmanipluate/#inspecting-data-with-head-and-tail","text":"Although cat command is an easy way for us to open and view the content of a file, it is not very practical to do so for a file with thousands of lines as it will exhaust the shell \"space\". Instead, large files should be inspected first and then manipulated accordingly. First round of inspection can be done with head and tail command which prints the first 10 lines and the last 10 lines ( -n 10 ) of a a file, respectively. .i.e. Let's use head and tail to inspect Mus_musculus.GRCm38.75_chr1.bed $ head Mus_musculus.GRCm38.75_chr1.bed 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733 1 3102016 3102125 1 3102016 3102125 1 3102016 3102125 1 3205901 3671498 1 3205901 3216344 1 3213609 3216344 1 3205901 3207317 $ tail Mus_musculus.GRCm38.75_chr1.bed 1 195166217 195166390 1 195165745 195165851 1 195165748 195165851 1 195165745 195165747 1 195228278 195228398 1 195228278 195228398 1 195228278 195228398 1 195240910 195241007 1 195240910 195241007 1 195240910 195241007 Changing the number of lines printed for either of those commands can be done by passing -n <number_of_lines> flag .i.e. Over-ride the -n 10 default Try those commands with 0n 4 to print top 4 lines and bottom 4 lines $ head -n 4 Mus_musculus.GRCm38.75_chr1.bed $ tail -n 4 Mus_musculus.GRCm38.75_chr1.bed Solution Check..Check check the status $ grep awk $ tail -n 4 Mus_musculus.GRCm38.75_chr1.bed $ head -n 4 Mus_musculus.GRCm38.75_chr1.bed Solution_2 Check..Check check the status $ grep awk $ tail -n 4 Mus_musculus.GRCm38.75_chr1.bed $ head -n 4 Mus_musculus.GRCm38.75_chr1.bed","title":"Inspecting data with head and tail"},{"location":"4_inspectmanipluate/#extract-summary-information-with-wc","text":"The \"wc\" in the wc command which stands for \"word count\" - this command can count the numbers of words, lines , and characters in a file (take a note on the order). $ wc Mus_musculus.GRCm38.75_chr1.bed 81226 243678 1698545 Mus_musculus.GRCm38.75_chr1.bed Often, we only need to list the number of lines, which can be done by using the -l flag. It can be used as a sanity check - for example, to make sure an output has the same number of lines as the input, OR to check that a certain file format which depends on another format without losing overall data structure wasn't corrupted or over/under manipulated. Question - count the number of lines in Mus_musculus.GRCm38.75_chr1.bed and Mus_musculus.GRCm38.75_chr1.gtf . Anything out of the ordinary ? Although wc -l is the quickest way to count the number of lines in a file, it is not the most robust as it relies on the very bad assumption that \"data is well formatted\" For an example, If we are to create a file with 3 rows of data and then two empty lines, $ cat > fool_wc.bed 1 100 2 200 3 300 $ $ wc -l fool_wc.bed 5 fool_wc.bed This is a good place bring in grep again which can be used to count the number of lines while excluding white-spaces (spaces, tabs or newlines) $ grep -c \"[^ \\\\n\\\\t]\" fool_wc.bed 3","title":"Extract summary information with wc"},{"location":"4_inspectmanipluate/#using-cut-with-column-data-and-formatting-tabular-data-with-column","text":"When working with plain-text tabular data formats like tab-delimited and CSV files, we often need to extract specific columns from the original file or stream. For example, suppose we wanted to extract only the start positions (the second column) of the Mus_musculus.GRCm38.75_chr1.bed file. The simplest way to do this is with cut . $ cut -f 2 Mus_musculus.GRCm38.75_chr1.bed | head -n 3 3054233 3054233 3054233 -f argument is how we specify which columns to keep. It can be used to specify a range as well $ cut -f 2 -3 Mus_musculus.GRCm38.75_chr1.bed | head -n 3 3054233 3054733 3054233 3054733 3054233 3054733 Using cut , we can convert our GTF for Mus_musculus.GRCm38.75_chr1.gtf to a three-column tab-delimited file of genomic ranges (e.g., chromosome, start, and end position). We\u2019ll chop off the metadata rows using the grep command covered earlier, and then use cut to extract the first, fourth, and fifth columns (chromosome, start, end): $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f1,4,5 | head -n 3 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733 Note that although our three-column file of genomic positions looks like a BED- formatted file, it\u2019s not due to subtle differences in genomic range formats cut also allows us to specify the column delimiter character. So, if we were to come across a CSV file containing chromosome names, start positions, and end positions, we could select columns from it, too: As you may have noticed when working with tab-delimited files, it\u2019s not always easy to see which elements belong to a particular column. For example: $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f1-8 | head -n3 1 pseudogene gene 3054233 3054733 . + . 1 unprocessed_pseudogene transcript 3054233 3054733 . + . 1 unprocessed_pseudogene exon 3054233 3054733 . + . While tabs are a great delimiter in plain-text data files, our variable width data leads our columns to not stack up well. There\u2019s a fix for this in Unix: program column -t (the -t option tells column to treat data as a table). column -t produces neat columns that are much easier to read: $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f 1 -8 | column -t | head -n 3 1 pseudogene gene 3054233 3054733 . + . 1 unprocessed_pseudogene transcript 3054233 3054733 . + . 1 unprocessed_pseudogene exon 3054233 3054733 . + . Note that you should only use column -t to visualize data in the terminal, not to reformat data to write to a file. Tab-delimited data is preferable to data delimited by a variable number of spaces, since it\u2019s easier for programs to parse. Like cut , column \u2019s default delimiter is the tab character ( \\t ). We can specify a different delimiter with the -s option. So, if we wanted to visualize the columns of the Mus_musculus.GRCm38.75_chr1_bed.csv file more easily, we could use: $ column -s \",\" -t Mus_musculus.GRCm38.75_chr1.bed | head -n 3 1 3054233 3054733 1 3054233 3054733 1 3054233 3054733","title":"Using cut with column data and formatting tabular data with column"},{"location":"4_inspectmanipluate/#sorting-plain-text-data-with-sort","text":"Very often we need to work with sorted plain-text data in bioinformatics. The two most common reasons to sort data are as follows: - Certain operations are much more efficient when performed on sorted data. - Sorting data is a prerequisite to finding all unique lines sort is designed to work with plain-text data with columns. Create a test .bed file with few rows and use sort command without any arguments $ cat > test_sort.bed chr1 26 39 chr1 32 47 chr3 11 28 chr1 40 49 chr3 16 27 chr1 9 28 chr2 35 54 chr1 10 19 $ sort test_sort.bed chr1 10 19 chr1 26 39 chr1 32 47 chr1 40 49 chr1 9 28 chr2 35 54 chr3 11 28 chr3 16 27 sort without any arguments simply sorts a file alphanumerically by line. Because chromosome is the first column, sorting by line effectively groups chromosomes together, as these are \"ties\" in the sorted order. However, using sort \u2019s defaults of sorting alphanumerically by line doesn\u2019t handle tabular data properly. There are two new features we need: The ability to sort by particular columns The ability to tell sort that certain columns are numeric values (and not alpha\u2010numeric text) sort has a simple syntax to do this. Let\u2019s look at how we\u2019d sort example.bed by chromosome (first column), and start position (second column): $ sort -k1,1 -k2n test_sort.bed chr1 9 28 chr1 10 19 chr1 26 39 chr1 32 47 chr1 40 49 chr2 35 54 chr3 11 28 chr3 16 27 Here, we specify the columns (and their order) we want to sort by as -k arguments. In technical terms, -k specifies the sorting keys and their order. Each -k argument takes a range of columns as start,end , so to sort by a single column we use start,start . In the preceding example, we first sorted by the first column (chromosome), as the first -k argument was -k1,1 . Sorting by the first column alone leads to many ties in rows with the same chromosomes (e.g., \u201cchr1\u201d and \u201cchr3\u201d). Adding a second -k argument with a different column tells sort how to break these ties. In our example, -k2,2n tells sort to sort by the second column (start position), treating this column as numerical data (because there\u2019s an n in -k2,2n ).","title":"Sorting Plain-Text Data with sort"},{"location":"4_inspectmanipluate/#exercise-43","text":"{% capture e4dot3 %} Mus_musculus.GRCm38.75_chr1_random.gtf file is Mus_musculus.GRCm38.75_chr1.gtf with permuted rows (and without a metadata header). Can you group rows by chromosome, and sort by position? If yes, append the output to a separate file. {% endcapture %} {% include exercise.html title=\"e4dot3\" content=e4dot3%}","title":"Exercise 4.3"},{"location":"4_inspectmanipluate/#finding-unique-values-with-uniq","text":"uniq takes lines from a file or standard input stream, and outputs all lines with consecutive duplicates removed. While this is a relatively simple functionality, you will use uniq very frequently in command-line data processing. $ cat letters.txt A A B C B C C C $ uniq letters.txt A B C B C As you can see, uniq does not return the unique values in letters.txt \u2014 it only removes consecutive duplicate lines (keeping one). If instead we did want to find all unique lines in a file, we would first sort all lines using sort so that all identical lines are grouped next to each other, and then run uniq . $ sort letters.txt | uniq A B C uniq with -c shows the counts of occurrences next to the unique lines. $ uniq -c letters.txt 2 A 1 B 1 C 1 B 3 C $ sort letters.txt | uniq -c 2 A 2 B 4 C Combined with other Unix tools like cut , grep and sort , uniq can be used to summarize columns of tabular data: $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f3 | sort | uniq -c 25901 CDS 36128 exon 2027 gene 2290 start_codon 2299 stop_codon 4993 transcript 7588 UTR Count in order from most frequent to last $ grep -v \"^#\" Mus_musculus.GRCm38.75_chr1.gtf | cut -f3 | sort | uniq -c | sort -rn 36128 exon 25901 CDS 7588 UTR 4993 transcript 2299 stop_codon 2290 start_codon 2027 gene \u00ab 3 - Shell Basics and recap 5 - Text Manipu. Pt2 \u00bb Back to homepage","title":"Finding Unique Values with uniq"},{"location":"5_inspectmanipulate2/","text":"Inspecting and Manipulating Text Data with Unix Tools - Part 2 \u00b6 Do not remove this line (it will not be displayed) Aho, Weinberger, Kernighan = AWK \u00b6 Awk is a scripting language used for manipulating data and generating reports. The awk command programming language requires no compiling and allows the user to use variables, numeric functions, string functions, and logical operators. Awk is a utility that enables a programmer to write tiny but effective programs in the form of statements that define text patterns that are to be searched for in each line of a document and the action that is to be taken when a match is found within a line. Awk is mostly used for pattern scanning and processing. It searches one or more files to see if they contain lines that matches with the specified patterns and then perform the associated actions. WHAT CAN WE DO WITH AWK? AWK Operations: Scans a file line by line Splits each input line into fields Compares input line/fields to pattern Performs action(s) on matched lines Useful For: Transform data files Produce formatted reports Programming Constructs: Format output lines Arithmetic and string operations Conditionals and loops Syntax : awk options 'selection_criteria {action}' input-file > output-file Options -f program-file : Reads the AWK program source from the file program-file, instead of from the first command line argument. -F fs : Use fs for the input field separator Default behaviour of awk is to print every line of data from the specified file. .i.e. mimics cat $ awk '{print}' example.bed chr1 26 39 chr1 32 47 chr3 11 28 chr1 40 49 chr3 16 27 chr1 9 28 chr2 35 54 chr1 10 19 Print lines which match the given pattern $ awk '/chr1/ {print}' example.bed chr1 26 39 chr1 32 47 chr1 40 49 chr1 9 28 chr1 10 19 awk can be used to mimic functionality of cut $ awk '{print $2 \"\\t\" $3}' example.bed 26 39 32 47 11 28 40 49 16 27 9 28 35 54 10 19 Here, we\u2019re making use of Awk\u2019s string concatenation. Two strings are concatenated if they are placed next to each other with no argument. So for each record, $2\"\\t\"$3 concatenates the second field, a tab character, and the third field. However, this is an instance where using cut is much simpler as the equivalent of above is cut -f2,3 example.bed Let\u2019s look at how we can incorporate simple pattern matching. Suppose we wanted to write a filter that only output lines where the length of the feature (end position - start position) was greater than 18. Awk supports arithmetic with the standard operators + , - , * , / , % (remainder), and ^ (exponentiation). We can subtract within a pattern to calculate the length of a feature, and filter on that expression: $ awk '$3 - $2 > 18' example.bed chr1 9 28 chr2 35 54 \u00b6 Comparison Description a == b a is equal to b a != b a is not equal to b a < b a is less than b a > b a is greater than b a <= b a is less than or equal to b a >= b a is greater than or equal to b a ~ b a matches regular expression pattern b a !~ b a does not match regular expression pattern b a && b logical a and b a \\|\\| b logical or a and b !a not a (logical negation) We can also chain patterns, by using logical operators && (AND), || (OR), and ! (NOT). For example, if we wanted all lines on chromosome 1 with a length greater than 10: $ awk '$1 ~ /chr1/ && $3 - $2 > 10' example.bed chr1 26 39 chr1 32 47 chr1 9 28 Built-In Variables and special patterns In Awk Awk\u2019s built-in variables include the field variables $1 , $2 , $3 , and so on ( $0 is the entire line) \u2014 that break a line of text into individual words or pieces called fields. NR : keeps a current count of the number of input records. Remember that records are usually lines. Awk command performs the pattern/action statements once for each record in a file. NF : keeps a count of the number of fields within the current input record. FS : contains the field separator character which is used to divide fields on the input line. The default is \u201cwhite space\u201d, meaning space and tab characters. FS can be reassigned to another character (typically in BEGIN) to change the field separator. RS : stores the current record separator character. Since, by default, an input line is the input record, the default record separator character is a newline. OFS : stores the output field separator, which separates the fields when Awk prints them. The default is a blank space. Whenever print has several parameters separated with commas, it will print the value of OFS in between each parameter. ORS : stores the output record separator, which separates the output lines when Awk prints them. The default is a newline character. print automatically outputs the contents of ORS at the end of whatever it is given to print. Also, there are two special patterns BEGIN & END BEGIN - specifies what to do before the first record is read in. Useful to initialise and set up variables END - what to do after the last record's processing is complete. Useful to print data summaries ad the end of file processing Examples We can use NR to extract ranges of lines, too; for example, if we wanted to extract all lines between 3 and 5 (inclusive): $ awk 'NR >= 3 && NR <=5' example.bed chr3 11 28 chr1 40 49 chr3 16 27 suppose we wanted to calculate the mean feature length in example.bed. We would have to take the sum feature lengths, and then divide by the total number of records. We can do this with: $ awk 'BEGIN{s = 0}; {s += ($3-$2)}; END{ print \"mean: \" s/NR};' example.bed mean: 14 In this example, we\u2019ve initialized a variable s to 0 in BEGIN (variables you define do not need a dollar sign). Then, for each record we increment s by the length of the feature. At the end of the records, we print this sum s divided by the number of records NR , giving the mean. awk makes it easy to convert between bioinformatics files like BED and GTF. For example, we could generate a three-column BED file from Mus_muscu\u2010 lus.GRCm38.75_chr1.gtf as follows: $ awk '!/^#/ { print $1 \"\\t\" $4-1 \"\\t\" $5}' Mus_musculus.GRCm38.75_chr1.gtf | head -n 3 1 3054232 3054733 1 3054232 3054733 1 3054232 3054733 * awk also has a very useful data structure known as an associative array. Associative arrays behave like Python\u2019s dictionaries or hashes in other languages. We can create an associative array by simply assigning a value to a key. For example, suppose we wanted to count the number of features (third column) belonging to the gene \u201cLypla1.\u201d We could do this by incrementing their values in an associative array: $ awk '/Lypla1/ {feature[$3] += 1}; END {for (k in feature) print k \"\\t\" feature[k]}' Mus_musculus.GRCm38.75_chr1.gtf CDS 56 transcript 9 start_codon 5 gene 1 exon 69 UTR 24 Back to homepage","title":"Inspecting and Manipulating Text Data with Unix Tools - Part 2"},{"location":"5_inspectmanipulate2/#inspecting-and-manipulating-text-data-with-unix-tools-part-2","text":"Do not remove this line (it will not be displayed)","title":"Inspecting and Manipulating Text Data with Unix Tools - Part 2"},{"location":"5_inspectmanipulate2/#aho-weinberger-kernighan-awk","text":"Awk is a scripting language used for manipulating data and generating reports. The awk command programming language requires no compiling and allows the user to use variables, numeric functions, string functions, and logical operators. Awk is a utility that enables a programmer to write tiny but effective programs in the form of statements that define text patterns that are to be searched for in each line of a document and the action that is to be taken when a match is found within a line. Awk is mostly used for pattern scanning and processing. It searches one or more files to see if they contain lines that matches with the specified patterns and then perform the associated actions. WHAT CAN WE DO WITH AWK? AWK Operations: Scans a file line by line Splits each input line into fields Compares input line/fields to pattern Performs action(s) on matched lines Useful For: Transform data files Produce formatted reports Programming Constructs: Format output lines Arithmetic and string operations Conditionals and loops Syntax : awk options 'selection_criteria {action}' input-file > output-file Options -f program-file : Reads the AWK program source from the file program-file, instead of from the first command line argument. -F fs : Use fs for the input field separator Default behaviour of awk is to print every line of data from the specified file. .i.e. mimics cat $ awk '{print}' example.bed chr1 26 39 chr1 32 47 chr3 11 28 chr1 40 49 chr3 16 27 chr1 9 28 chr2 35 54 chr1 10 19 Print lines which match the given pattern $ awk '/chr1/ {print}' example.bed chr1 26 39 chr1 32 47 chr1 40 49 chr1 9 28 chr1 10 19 awk can be used to mimic functionality of cut $ awk '{print $2 \"\\t\" $3}' example.bed 26 39 32 47 11 28 40 49 16 27 9 28 35 54 10 19 Here, we\u2019re making use of Awk\u2019s string concatenation. Two strings are concatenated if they are placed next to each other with no argument. So for each record, $2\"\\t\"$3 concatenates the second field, a tab character, and the third field. However, this is an instance where using cut is much simpler as the equivalent of above is cut -f2,3 example.bed Let\u2019s look at how we can incorporate simple pattern matching. Suppose we wanted to write a filter that only output lines where the length of the feature (end position - start position) was greater than 18. Awk supports arithmetic with the standard operators + , - , * , / , % (remainder), and ^ (exponentiation). We can subtract within a pattern to calculate the length of a feature, and filter on that expression:","title":"Aho, Weinberger, Kernighan = AWK"},{"location":"5_inspectmanipulate2/#awk-3-2-18-examplebed-chr1-9-28-chr2-35-54","text":"Comparison Description a == b a is equal to b a != b a is not equal to b a < b a is less than b a > b a is greater than b a <= b a is less than or equal to b a >= b a is greater than or equal to b a ~ b a matches regular expression pattern b a !~ b a does not match regular expression pattern b a && b logical a and b a \\|\\| b logical or a and b !a not a (logical negation) We can also chain patterns, by using logical operators && (AND), || (OR), and ! (NOT). For example, if we wanted all lines on chromosome 1 with a length greater than 10: $ awk '$1 ~ /chr1/ && $3 - $2 > 10' example.bed chr1 26 39 chr1 32 47 chr1 9 28 Built-In Variables and special patterns In Awk Awk\u2019s built-in variables include the field variables $1 , $2 , $3 , and so on ( $0 is the entire line) \u2014 that break a line of text into individual words or pieces called fields. NR : keeps a current count of the number of input records. Remember that records are usually lines. Awk command performs the pattern/action statements once for each record in a file. NF : keeps a count of the number of fields within the current input record. FS : contains the field separator character which is used to divide fields on the input line. The default is \u201cwhite space\u201d, meaning space and tab characters. FS can be reassigned to another character (typically in BEGIN) to change the field separator. RS : stores the current record separator character. Since, by default, an input line is the input record, the default record separator character is a newline. OFS : stores the output field separator, which separates the fields when Awk prints them. The default is a blank space. Whenever print has several parameters separated with commas, it will print the value of OFS in between each parameter. ORS : stores the output record separator, which separates the output lines when Awk prints them. The default is a newline character. print automatically outputs the contents of ORS at the end of whatever it is given to print. Also, there are two special patterns BEGIN & END BEGIN - specifies what to do before the first record is read in. Useful to initialise and set up variables END - what to do after the last record's processing is complete. Useful to print data summaries ad the end of file processing Examples We can use NR to extract ranges of lines, too; for example, if we wanted to extract all lines between 3 and 5 (inclusive): $ awk 'NR >= 3 && NR <=5' example.bed chr3 11 28 chr1 40 49 chr3 16 27 suppose we wanted to calculate the mean feature length in example.bed. We would have to take the sum feature lengths, and then divide by the total number of records. We can do this with: $ awk 'BEGIN{s = 0}; {s += ($3-$2)}; END{ print \"mean: \" s/NR};' example.bed mean: 14 In this example, we\u2019ve initialized a variable s to 0 in BEGIN (variables you define do not need a dollar sign). Then, for each record we increment s by the length of the feature. At the end of the records, we print this sum s divided by the number of records NR , giving the mean. awk makes it easy to convert between bioinformatics files like BED and GTF. For example, we could generate a three-column BED file from Mus_muscu\u2010 lus.GRCm38.75_chr1.gtf as follows: $ awk '!/^#/ { print $1 \"\\t\" $4-1 \"\\t\" $5}' Mus_musculus.GRCm38.75_chr1.gtf | head -n 3 1 3054232 3054733 1 3054232 3054733 1 3054232 3054733 * awk also has a very useful data structure known as an associative array. Associative arrays behave like Python\u2019s dictionaries or hashes in other languages. We can create an associative array by simply assigning a value to a key. For example, suppose we wanted to count the number of features (third column) belonging to the gene \u201cLypla1.\u201d We could do this by incrementing their values in an associative array: $ awk '/Lypla1/ {feature[$3] += 1}; END {for (k in feature) print k \"\\t\" feature[k]}' Mus_musculus.GRCm38.75_chr1.gtf CDS 56 transcript 9 start_codon 5 gene 1 exon 69 UTR 24 Back to homepage","title":"$ awk &#39;$3 - $2 &gt; 18&#39; example.bed \nchr1    9   28\nchr2    35  54\n"}]}